{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92aa8e1a",
   "metadata": {},
   "source": [
    "### Value Counts of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f26814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/HateSpeechDatasetBalanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982f86d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    364525\n",
       "0    361594\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccdc9d5",
   "metadata": {},
   "source": [
    "### Make training samples for training and validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a8c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_0_test = df[df['Label'] == 0].iloc[:50000]\n",
    "df_label_1_test = df[df['Label'] == 1].iloc[:50000]\n",
    "df_test = pd.concat([df_label_0_test, df_label_1_test])\n",
    "\n",
    "df_label_0_train1 = df[df['Label'] == 0].iloc[50000:200000]\n",
    "df_label_1_train1 = df[df['Label'] == 1].iloc[50000:200000]\n",
    "df_train1 = pd.concat([df_label_0_train1, df_label_1_train1])\n",
    "\n",
    "df_label_0_train2 = df[df['Label'] == 0].iloc[50000:]\n",
    "df_label_1_train2 = df[df['Label'] == 1].iloc[50000:]\n",
    "df_train2 = pd.concat([df_label_0_train2, df_label_1_train2])\n",
    "\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_train1 = df_train1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_train2 = df_train2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_test.to_csv('../data/test_dataset.csv', index=False)\n",
    "df_train1.to_csv('../data/train_dataset_1.csv', index=False)\n",
    "df_train2.to_csv('../data/train_dataset_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05574af3",
   "metadata": {},
   "source": [
    "### Preprocessing data, train and evaluate Pyspark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887922b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "# Инициализация сессии Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text Preprocessing with Lemmatization in PySpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.storage.memoryFraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Шаг 1: Загрузка данных\n",
    "data_path = \"s3a://amamylov-mlops/hate_speech_detection/train_dataset_1.csv\"\n",
    "data = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Шаг 2: Предобработка данных — удаление пустых строк\n",
    "data = data.select(\"Content\", \"Label\").na.drop()\n",
    "\n",
    "# Шаг 3: Определение этапов препроцессинга\n",
    "# Токенизация\n",
    "tokenizer = Tokenizer(inputCol=\"Content\", outputCol=\"words\")\n",
    "\n",
    "# Удаление стоп-слов\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Индексация меток\n",
    "indexer = StringIndexer(inputCol=\"Label\", outputCol=\"indexedLabel\")\n",
    "\n",
    "# RandomForest классификатор\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=100, maxDepth=15)\n",
    "\n",
    "# Шаг 4: Создание и обучение Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, remover, hashing_tf, idf, rf])\n",
    "\n",
    "# Разделение данных на тренировочные и тестовые выборки\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Обучение модели\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# Шаг 5: Предсказание на тестовых данных\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# Шаг 6: Оценка метрик\n",
    "# F1, Precision, Recall\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision_score = evaluator_precision.evaluate(predictions)\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall_score = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "# ROC AUC\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"indexedLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc_score = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "# Вывод метрик\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Precision: {precision_score}\")\n",
    "print(f\"Recall: {recall_score}\")\n",
    "print(f\"ROC AUC: {roc_auc_score}\")\n",
    "\n",
    "# Шаг 7: Сохранение Pipeline целиком\n",
    "model_path = \"s3a://amamylov-mlops/hate_speech_detection/model\"\n",
    "pipeline_model.save(model_path)\n",
    "\n",
    "print(f\"Pipeline сохранен в {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddfb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_label_index = 1\n",
    "results = predictions.select(\"Content\", \"indexedLabel\", \"prediction\")\n",
    "toxic_comments = results.filter(col(\"prediction\") == toxic_label_index)\n",
    "toxic_comments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Путь к сохраненному пайплайну\n",
    "model_path = \"s3a://amamylov-mlops/hate_speech_detection/model\"\n",
    "\n",
    "# Загрузка сохраненной модели\n",
    "pipeline_model = PipelineModel.load(model_path)\n",
    "\n",
    "# Пример нового комментария для предсказания\n",
    "new_comment = \"This is a good comment!\"\n",
    "\n",
    "# Шаг 1: Создаем DataFrame с новым комментарием\n",
    "data = [Row(Content=new_comment)]\n",
    "new_data_df = spark.createDataFrame(data)\n",
    "\n",
    "# Шаг 2: Выполняем предсказание\n",
    "predictions = pipeline_model.transform(new_data_df)\n",
    "\n",
    "# Шаг 3: Извлекаем предсказания и вероятности\n",
    "predicted_label = predictions.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "probability = predictions.select(\"probability\").collect()[0][\"probability\"]\n",
    "\n",
    "# Отображение результата\n",
    "print(f\"Комментарий: '{new_comment}'\")\n",
    "print(f\"Классифицирован как: {'негативный' if predicted_label == 1.0 else 'позитивный'}\")\n",
    "print(f\"Вероятности классов: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
